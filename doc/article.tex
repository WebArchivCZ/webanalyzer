% This percent indicates a comment.
% This is a very simple latex article that introduces the way 
% equations are typeset.  Do this in linux:
%
% latex first.tex
% latex first.tex
% xdvi first.dvi
% dvips -o first.ps first.dvi
% gv first.ps
% lpr first.ps
% pdflatex first.tex
% acroread first.pdf
\documentclass[11pt,a4paper]{article}
\usepackage[cp1250]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{url}

\begin{document}

\begin{center}
{\huge Identification and Archiving of the Czech Web Outside the National Domain}

\vspace{3.2em}

{\LARGE Ivan Vlƒçek}

\newpage
\tableofcontents

\end{center}
\newpage

%================================NEW SECTION================================%

\newpage
\section{Introduction}

The placement of different documents on the Internet has become very popular recently todo(has become recently ?). Internet users find this feature helpfull, because new documents can be uploaded on the Internet in relatively small prices of time and effort.  Some of these documents are considered to be very important in spheres such as history, science and many many others. But there exists a risk, that the information will be changed, or even deleted and lost forever. Therefore it's our duty to save this sort of documents, that carry important information, so as to be available (todo check) for future generation.

The question of archiving the Internet content is solved by many institutions. The most famous is called Internet Archive, that creates Internet library and provides access to this library.

There is a project called WebArchiv in Czech Republic, that manages archiving and ensures access to archival collection. The only criterion how to identify the czech webpage operates on the basis of occurrence of domain, that is included in URL. This ensures all URLs with czech domain are archived. But there are also websites considered to be czech, whose domain is different from cz. WebArchiv has strong interest in these websites and therefore one of the latest goal is archiving the czech web outside the national domain.

This work describes the system called WebAnalyzer to identify and archive the czech web outside the national domain. The system identifying czech web is integrated in open-source software Heritrix, that is able to archive these webpages. With this system it is possible to identify the Bohemian resource on the basis of many criteria that are included in the system. Bohemian source is every document that meets conditions defined by user. The user is able to set these conditions before starting Heritrix, so that the conditions meet his demands for Bohemian source. These conditions are valuated by criteria that are implemented in the system. Criteria contains identifying of language, czech IP addresses, czech e-mails and many others. Each webpage identified as Bohemian page is than achived by Heritrix.

%================================NEW SECTION================================%

\newpage
\section{WebArchiv project}

Main task of the project WebArchiv is archiving and providing access to Czech web. WebArchiv uses web crawlers to perform comprehensive harvesting of national domain. We have been using Hertrix recently(todo cas), because of it's advantages. Heritrix is modular, extendible and it's easy to configure the crawl job. You're able to set those modules, that meets your needs. And if you demand some new functionality, it's possible to develop new module and attach it to Heritrix.

\subsection{The comprehensive harvesting of Czech web}
Heritrix performs the comprehensive harvesting on basis of modules, that filters out all URIs whose domain differs from .cz. These filtered URIs are called out of scope links and they're ignored by crawl job. All Czech webpages, that pass through the process of crawl job are archived. Links discovered in the content of processed webpages are filtered on the basis of their domain. Discovered links with czech domain are further included into collection of URIs, waiting for being processed, while other links with different domains are ignored and marked as out of scope.

\subsection{The comprehensive harvesting of Czech web outside the national domain}
Harvesting of Czech web outside the national domain should follow the harvesting of Czech web. 

Out of scope links, that comes from harvesting of Czech web are valuable input material for following harvesting of web outside the national domain. There is a chance, that these links reffered from czech webpages lead to Bohemian sites. Therefore it's important to start crawling with these URIs. Out of scope links are set as seeds for new crawl job of harvesting outside the national domain.

%================================NEW SECTION================================%

\newpage
\section{Heritrix}

Heritrix is able to do both, crawl web and archive it. It's open-source software written in Java, that provides user-friendly web interface. User manual is available on http://crawler.archive.org. Before we try to describe new system integrated in Heritrix it's important to know how Heritrix works.

\subsection{A brief overview}
Heritrix is designed to be modular. Which modules to use can be set at runtime from the user interface. If you demand new functionality, you can implement new module and attach it to Heritrix, or you can replace existing module.

The crawler consists of core classes and pluggable modules. The core classes can be configured, but not replaced. The pluggable classes can be substituted by altering the configuration of the crawler. A set of basic pluggable classes are shipped with the crawler, but if you have needs not met by these classes you could write your own.

\includegraphics[width=120mm]{crawler_overview1.png}

\subsection{The CrawlController}
The CrawlController collects all the classes which cooperate to perform a crawl, provides a high-level interface to the running crawl, and executes the "master thread" which doles out URIs from the Frontier to the ToeThreads. As the "global context" for a crawl, subcomponents will usually reach each other through the CrawlController.

\subsection{The Frontier}
The Frontier is responsible for handing out the next URI to be crawled. It is responsible for maintaining politeness, that is making sure that no web server is crawled too heavily. After a URI is crawled, it is handed back to the Frontier along with any newly discovered URIs that the Frontier should schedule for crawling.

It is the Frontier which keeps the state of the crawl. This includes, but is not limited to:

\begin{itemize}
\item What URIs have been discovered
\item What URIs are being processed (fetched)
\item What URIs have been processed
\end{itemize}

The Frontier implements the Frontier interface and can be replaced by any Frontier that implements this interface. It should be noted though that writing a Frontier is not a trivial task.

The Frontier relies on the behavior of at least the following external processors: PreconditionEnforcer, LinksScoper and the FrontierScheduler (See below for more each of these Processors). The PreconditionEnforcer makes sure dns and robots are checked ahead of any fetching. LinksScoper tests if we are interested in a particular URL -- whether the URL is 'within the crawl scope' and if so, what our level of interest in the URL is, the priority with which it should be fetched. The FrontierScheduler adds ('schedules') URLs to the Frontier for crawling.

\subsection{ToeThreads}
The Heritrix web crawler is multi threaded. Every URI is handled by its own thread called a ToeThread. A ToeThread asks the Frontier for a new URI, sends it through all the processors and then asks for a new URI.

\subsection{Processors}
Processors are grouped into processor chains. Each chain does some processing on a URI. When a Processor is finished with a URI the ToeThread sends the URI to the next Processor until the URI has been processed by all the Processors. A processor has the option of telling the URI to skip to a particular chain. Also if a processor throws a fatal error, the processing skips to the Post-processing chain.

\includegraphics[width=40mm]{processing_steps.png}

The task performed by the different processing chains are as follows:

\subsubsection{Pre-fetch processing chain}
The first chain is responsible for investigating if the URI could be crawled at this point. That includes checking if all preconditions are met (DNS-lookup, fetching robots.txt, authentication). It is also possible to completely block the crawling of URIs that have not passed through the scope check.

In the Pre-fetch processing chain the following processors should be included (or replacement modules that perform similar operations):

\begin{description}
\item[Preselector] Last check if the URI should indeed be crawled. Can for example recheck scope. Useful if scope rules have been changed after the crawl starts. The scope is usually checked by the LinksScoper, before new URIs are added to the Frontier to be crawled. If the user changes the scope limits, it will not affect already queued URIs. By rechecking the scope at this point, you make sure that only URIs that are within current scope are being crawled.
\item[PreconditionEnforcer] Ensures that all preconditions for crawling a URI have been met. These currently include verifying that DNS and robots.txt information has been fetched for the URI.
\end{description}

\subsubsection{Fetch processing chain}
The processors in this chain are responsible for getting the data from the remote server. There should be one processor for each protocol that Heritrix supports: e.g. FetchHTTP.

\subsubsection{Extractor processing chain}
At this point the content of the document referenced by the URI is available and several processors will in turn try to get new links from it.

\subsubsection{Write/index processing chain}
This chain is responsible for writing the data to archive files. Heritrix comes with an ARCWriterProcessor which writes to the ARC format. New processors could be written to support other formats and even create indexes.

\subsubsection{Post-processing chain}
A URI should always pass through this chain even if a decision not to crawl the URI was done in a processor earlier in the chain. The post-processing chain must contain the following processors (or replacement modules that perform similar operations):

\begin{description}
\item[CrawlStateUpdater] Updates the per-host information that may have been affected by the fetch. This is currently robots and IP address info.
\item[LinksScoper] Checks all links extracted from the current download against the crawl scope. Those that are out of scope are discarded. Logging of discarded URLs can be enabled.
\item[FrontierScheduler] 'Schedules' any URLs stored as CandidateURIs found in the current CrawlURI with the frontier for crawling. Also schedules prerequisites if any.
\end{description}

Further information are available in developer manual \url{http://crawler.archive.org/articles/developer_manual/index.html}.

%================================NEW SECTION================================%

\newpage
\section{Design and Integration}

It's very important to integrate the WebAnalyzer into the Heritrix properly for us to gain required behaviour. The WebAnalyzer must analyze every URI, that goes through the process of crawling. Therefore it's necessary to integrate WebAnalyzer into some module in Heritrix. 

A good idea is to implement the WebAnalyzer as a standalone module, that provides defined inteface, that will be used by some of Heritrix's module. The simple input for WebAnalyzer should be the URI, about which it decides whether it is Bohemian URI or isn't. The output should be the logical value (true if analyzed URI is marked as Bohemian URI and false otherwise). The next important thing, is to choose the proper Heritrix's module, into which we integrate the WebAnalyzer.

The WebAnalyzer should start analyzing the URI, after Heritrix gets the biggest possible amount of information about processed URI.

% integration into Frontier
Integration into Frontier is not possible due to two reasons. Frontier has access to processed URI at the very beginning of crawling or at the very end. At the beginning there are almost no discovered information about URI. At the end of processing Heritrix discovered much information about URI, but at this point it's not possible to archive Bohemian URI. Archiving is executed by ARCWriterProcessor that archives URI before it finishes in Frontier. Both of these possibilities aren't convenient. We have to integrate the WebAnalyzer into module, that will be executed before ARCWriterProcessor, so that we can decide whether the processed URI has to be archived.

% integration into Filter
Integration into module Filter seemed to be conveniet, because Filter can be placed at any point during the process of crawling. Filter checks whether processed URL meets all defined criteria. It would be easy to place the Filter before ARCWriterProcessor and use the WebAnalyzer in Filter to identify the URI. But later there was a problem with initialization. WebAnalyzer must be initialized before it can be used, because it uses some databases and files that must be initialized. The Filter's interface doesn't provide the method to do this.

% integration into Scope
Integration into module Scope meets requirements for initializing and closing the WebAnalyzer instance. But Interne Archive posted, that the Scope module will be reorganized in the next version of Heritrix, so we tried another possibility.

% integration into Processor
Integration into module Processor was the final and right possibility. During testing the first version it was clear, that the integration into Processor is necessary. Processors are grouped in processor chains as we described before. The right chain for WebAnalyzer is extractor processing chain. At this point the content of the document referenced by the URI is available and several processors such as ExtractorHTML and ExtractorCSS discovered new links from it. Therefore we have to implement new Extractor, that will be placed at the end of extractor processing chain, when all information (content type, text content, discovered links), will be available. Next chain is write/index processing chain that archives URI and last chain is called post-processing chain, that filters out discovered links according to definition set by user. Having been processed by post-processing chain, the URI finishes in Frontier again. All URI's discovered links are scheduled in Frontier and at this point the crawling of URI finishes

The Extractor provides two methods initialize() and finalize(). All Processors are initialized by initialize() method at the beginning of the crawling and at the end the finalize() method is called by ToeThread to finalze all used Processors. We can simply use these methods to initialize and close WebAnalyzer. In the main method of Extractor, called innerProcess(CrawlURI curi), we can use the WebAnalyzer to identify the processed CrawlURI. Only CrawlURI identified as Bohemian is archived by ARCWriterProcessor.

The first prototype was implemented and tested. During testing we found some new requirements, because prototype wasn't able to archive images, css styles, links and other sources placed on the Bohemian URI. The integration must be done by more modules.

\subsection{The integration by few modules}
On the basis of new requirements, the use case diagram was created. The use case diagram describes intregration of WebAnalyzer into Heritrix. The integration is realized by modules ExtractorWebAnalyzer, ARCWriterProcessorWebAnalyzer and LinksScoperWebAnalyzer. The ExtractorWebAnalyzer is the only one, that directly uses the interface of WebAnalyzer module.

\includegraphics[width=120mm]{usecase1.png}

\subsubsection{The archiving of DNS records}
The Heritrix automaticaly archives all DNS records, before processing any webpage. We don't need all these DNS records but only those of Bohemian URI. It's hard to reach this state, therefore we use our ARCWriterProcessorWebAnalyzer, that simply ignores all DNS records so far.

\subsubsection{The Mime type detector}
During testing the first version, we noticed many OutOfMemeory exceptions, that was caused by analyzing the documents, the Mime type of which was wrong detected by Heritrix.

Old servers unable to identify the type of document, set text/plain value as Mime type for each document. WebAnalyzer analyzed only documents of text type, but actually they could be audio or video documents, the size of which can take hundreds of MB. This led to creating big java.lang.String objects, that caused OutOfMemory exceptions. The functionality of Heritrix to identify the content type of document is not sufficient and therefore we have to implement some sort of Mime detector, that will identify the content type of document before it will be analyzed by WebAnalyzer. This ensures that documents of binary content won't be analyzed by WebAnalyzer.

\subsubsection{The archiving of links and images}
The first version archives only Bohemian URI. Other URIs (images, css, other sources) associated with this Bohemian URI are not archived. But we need to archive not only sources associated with Bohemian URI but also it's links. Links referenced from Bohemian URI can carry worth information and it's good idea to archive them to defined level from Bohemian URI.

\includegraphics[width=120mm]{depth.png}

Picture shows a tree with Bohemian URI as root. The nodes are links referenced from Bohemian URI (images, css styles, other html pages). If the defined depth equals 2, than all links have to be archived to this level (as shows picture). If the document on the second level is html page, than it will be archived without it's images, css styles and other sources. Therefore we have to archive all documents from the third level except of html documents. This ensures that all images and other sources placed on archived html page will be achived as well. User is able to define the depth's value before starting the crawl job of Heritrix.

Realization of archiving links and images required new module LinksScoperWebAnalyzer and some changes in ExtractorWebAnalyzer, so that these modules could cooperate together. Objects CrawlURI and CandidateURI are able to remember any information during the process of their crawling. And this is the way how we force the Heritrix to maintain the state of archiving the links and images. Module LinksScoperWebAnalyzer extends original module LinksScoper, but adds the functionality to handle links referenced from Bohemian URI.

Lets describe the workflow of archiving links from Bohemian URI:

\includegraphics[width=120mm]{archiveLinks.png}

\begin{description}
\item[1. Frontier next(int timeout)] - Each CrawlURI enters the process of crawling from Frontier, by calling his method next() by ToeThread. The CrawlURI than goes through processors.
\item[2. ExtractorWebAnalyzer innerProcess(CrawlURI curi)] - When CrawlURI reaches the ExtractorWebAnalyzer, the CrawlURI is checked whether it contains some flag. Next processing depedns on the flag's value.

\begin{description}
\item[2.1 CrawlURI NO FLAG] - The CrawlURI has no flag. WebAnalyzer identifies it's mime type with MimeTypeDetector.
\item[2.2 CrawlURI FLAG=valid\_link, DEPTH=x] - The CrawlURI has the flag valid\_link determining that this CrawlURI was referenced from Bohemian URI and it has to be archived immediately. WebAnalyzer doesn't analyze this CrawlURI, it would do it in vain. The CrawlURI is sent directly to ARCWriterProcessorWebAnalyzer to be archived. The DEPTH=x determines the level, to which the links discovered on this CrawlURI has to be archived as well.
\item[2.3 CrawlURI FLAG=archive\_bin\_link, DEPTH=0] - The CrawlURI with this flag determines, that this URI is referenced from Bohemian URI. We have to archive all CrawlURIs with this flag except of html pages to ensure that all images and other sources referenced from Bohemian URI will be archived. See picture depth,... todo. CrawlURIs with this flag represent the lists of tree.

\begin{description}
\item[2.3.1 CrawlURI Content type=text/css, DEPTH=0] - If the CrawlURI with flag archive\_bin\_link is a document of type text/css, we have to archive it immediately and also set new flag valid\_link, DEPTH=0 for this CrawlURI. This ensures that all links referenced from css document will be archive as well. 
\item[2.3.2 CrawlURI Content type=other] - CrawlURI with flag archive\_bin\_link, the content type of which differs from text/html, has to be archived. This ensures all multimedia sources from Bohemian URI will be in arhival collection.
\item[2.3.3 CrawlURI Content type=text/html] - CrawlURI with archive\_bin\_link flag, the content type of which is text/html will not be archived. This ensures that the depth defined by user will not be exceeded.
\end{description}
\end{description}


\item[3. MimeTypeDetector isContentTypeText(curi.getName())] - The CrawlURI with no flag is identified by MimeTypeDetector, that is a part of WebAnalyzer module. According to the identified mime type, the next step is chosen.

\begin{description}
\item[3.1 CrawlURI Content type=other] - The CrawlURI which identified mime type is different from text will not be analyzed. Probably it's a URI with binary content. This CrawlURI skips to post-processing chain.
\item[3.2 CrawlURI Content type=text] - The CrawlURI with text content type will be analyzed by WebAnalyzer in next step.
\end{description}

\item[4. WebAnalyzer run(curiName, outlinks, contentType, content)] - All CrawlURIs in this branch are analyzed by WebAnalyzer. On the basis of CrawlURI's details, WebAnalyzer decides whether the CrawlURI is Bohemian URI or isn't.

\begin{description}
\item[4.1 CrawlURI NO FLAG] - CrawlURI is not identified as Bohemian URI. It skips to post-processing chain.
\item[4.2 CrawlURI FLAG=valid\_URI, DEPTH=x] - The CrawlURI is identified as Bohemian URI. The flag and depth attribute are associated with this CrawlURI.
\end{description}

\item[5. ARCWriterProcessorWebAnalyzer innerProcess(CrawlURI curi)] - This processor archives all CrawlURIs except of DNS records.
\item[6. LinksScoperWebAnalyzer innerProcess(CrawlURI curi)] - This processor handles all links discovered on CrawlURI. New CandidateURIs are created from discovered links and corresponding flags are associated with CandidateURIs.

\begin{description}
\item[6.1 CrawlURI NO FLAG] - The CrawlURI with no flag is processed in usual way.

\begin{description}
\item[6.1.2 createCandidateURI()] - The CandidateURI object is created from link.
\item[6.1.3 CandidateURI NO FLAG] - The CandidateURI whose parent CrawlURI doesn't contain any flag, has no flag as well.
\end{description}

\item[6.2 CrawlURI FLAG=valid\_link, DEPTH=x] - The CrawlURI with this flag is next processed according to the value of DEPTH attribute.

\begin{description}
\item[6.2.1 createCandidateURI()] - The CandidateURI object is created from link.
\item[6.2.2 CandidateURI FLAG=valid\_link, DEPTH=x-1] - The CrawlURI with depth value x>0. Flag valid\_link, DEPTH=x-1, is set for newly created CandidateURI. When the depth value is x=0, then the archive\_bin\_link is associated with CandidateURI. This ensures that valid links from Bohemian URI will be archived until (todo unless) they reach defined depth.
\item[6.2.3 CandidateURI FLAG=archive\_bin\_link] - The CrawlURI with depth value x=0. The flag archive\_bin\_link is associated with CandidateURI. This flag ensures, that all sources (images, css, ...) placed on the archived URI will be archived as well.
\end{description}


\item[6.3 CralURI FLAG=valid\_URI, DEPTH=x] - The CrawlURI with this flag is next processed according to the value of depth attribute.

\begin{description}
\item[6.3.1 createCandidateURI()] - The CandidateURI object is created from link.
\item[6.3.2 CandidateURI FLAG=valid\_link, DEPTH=x-1] - The CrawlURI with depth value x>0. Flag valid\_link, DEPTH=x-1 will be associated with CandidateURI.
\item[6.3.3 CandidateURI FLAG=arhchive\_bin\_link] - The CrawlURI with depth value x=0. Flag archive\_bin\_link will be associated with CandidateURI.
\end{description}
\end{description}

\item[7. Post-processor chain] - Each CrawlURI must go through this processor chain.
\item[8. Frontier schedule(CandidateURI caURI)] - Schedules each CandidateURI to the collection of waiting URIs, according to the priority.
\item[9. Frontier finish(CrawlURI curi)] - After the CrawlURI is processed by all processors, the ToeThread calls finish method of Frontier object. This ToeThread is now free to process next URI.

\end{description}

%================================NEW SECTION================================%

\newpage
\section{WebAnalyzer - Design and Implementation}

WebAnalyzer is realized as standalone module packaged into .jar file. WebAnalyzer provides simple interface, that is used by ExtractorWebAnalyzer to identify the processed URI. The rest of the functionality is implemented in integration modules. WebAnalyzer identifies the processed URI on the basis of properties defined by user. These properties can be defined in external file webanalyzer.properies. User is able to configure which modules of WebAnalyzer he wants to use and define conditions for valid URI.

WebAnalyzer is designed to be modular, so that new modules can be added easily. WebAnalyzer provides interfaces for new modules, so it is easy to implement new criterion to identifying analyzed URI.

\subsection{Overview of class diagram}

WebAnalyzer consists of few packages.

\includegraphics[width=120mm]{webanalyzerCD.png}

Lets describe the packages:




%================================NEW SECTION================================%

\newpage
\section{Testing}

all from bachelour work

%================================NEW SECTION================================%

\newpage
\section{Conclusion}

Conclusion

%================================NEW SECTION================================%

\newpage
\section{Sources}

\begin{itemize}
\item http://lucene.apache.org
\item http://www.vsj.co.uk/java/display.asp?id=474
\end{itemize}


\begin{itemize}
\item Pri vÔøΩvoji rÔøΩznych softwÔøΩrov sa vÔøΩÔøΩÔøΩina vÔøΩvojÔøΩrov ihneÔøΩ uchyÔøΩuje k relaÔøΩnÔøΩm databÔøΩzam.
\item Napriek tomu, ÔøΩe relaÔøΩnÔøΩ databÔøΩza je jednoducho prÔøΩsptupnÔøΩ, vyspelÔøΩ a robustnÔøΩ, je to technolÔøΩgia, ktorÔøΩ bola pÔøΩvodne navrhnutÔøΩ
pre vysoko ÔøΩtrukturovanÔøΩ dÔøΩta.
\item Existuje vÔøΩak mnoho reÔøΩlnych dÔøΩt, ktorÔøΩ sa jednoducho nehodia do ÔøΩtrutktÔøΩrovanej organizÔøΩcie (dokumenty, webovÔøΩ strÔøΩnky, e-maily...)
\item Na efektÔøΩvnu persistenciu tÔøΩchto dÔøΩt, ktorÔøΩ umoÔøΩnÔøΩ rÔøΩchle a ÔøΩahkÔøΩ vyhÔøΩadÔøΩvanie, mÔøΩÔøΩete dÔøΩta uloÔøΩiÔøΩ do relaÔøΩnej databÔøΩzy alebo mÔøΩÔøΩete pouÔøΩiÔøΩ jednoduchÔøΩiu a moÔøΩno vÔøΩkonnejÔøΩiu alternatÔøΩvu.
\item Takouto alternatÔøΩvou mÔøΩÔøΩe byÔøΩ open-source projekt Lucene search engine od Apache Souftware Foundation.
\end{itemize}

\section{O projekte Apache Lucene}

\begin{itemize}
\item Apache Lucene je vysoko vÔøΩkonnÔøΩ, full-textovÔøΩ vyhÔøΩadÔøΩvacÔøΩ nÔøΩstroj, napÔøΩsanÔøΩ v Jave. Je to technolÔøΩgia vhodnÔøΩ pre takmer kaÔøΩdÔøΩ aplikÔøΩciu, ktorÔøΩ vyÔøΩaduje full-textovÔøΩ vyhÔøΩadÔøΩvanie.
\item Je to open-source projekt verejne prÔøΩstupnÔøΩ na strÔøΩnke http://lucene.apache.org/
\item Projekt Lucene patrÔøΩ k poprednÔøΩm projektom Apache Software Foundation. To znamenÔøΩ, ÔøΩe je zÔøΩujem ho naÔøΩalej podporovaÔøΩ a vyvÔøΩjaÔøΩ. VhodnÔøΩ na pouÔøΩitie do aplikÔøΩciÔøΩ, bude naÔøΩalej podporovanÔøΩ aj v budÔøΩcnosti.
\item FyzickÔøΩ podoba Lucene je vo forme Java APIs. MÔøΩÔøΩete ju pouÔøΩiÔøΩ na indexovanie a hÔøΩadanie vo veÔøΩkÔøΩch blokoch textovÔøΩch dÔøΩt, vrÔøΩtane vytvorenia full-textovÔøΩho indexu.
\item Full-textovÔøΩ indexovanie nÔøΩm ponÔøΩka vyhÔøΩadaÔøΩ hocijakÔøΩ slovÔøΩ, frÔøΩzy alebo fragmenty viet v celom textovom obsahu.
\end{itemize}

\newpage

\section{ZÔøΩkladnÔøΩ koncepty Lucene}

\begin{itemize}
\item KniÔøΩnica Lucene vytvÔøΩra a spravuje indexy.
\item V Lucene je index chÔøΩpanÔøΩ ako seqencia Dokumentov. Document je ÔøΩpecifickÔøΩ Java trieda, ktorÔøΩ pozostÔøΩva z mnoÔøΩiny polÔøΩ.

\includegraphics[width=120mm]{depth.png}

\item Pri pouÔøΩitÔøΩ Lucene mÔøΩÔøΩete indexovaÔøΩ blok textu vytvorenÔøΩm objektu Dokument, predanÔøΩm vÔøΩetkÔøΩch instanciÔøΩ vytvorenÔøΩch dokumentov objektu
IndexWriter. Trieda IndexWriter vytvÔøΩra a spravuje indexy.
\item NaprÔøΩklad ak chcete indexovaÔøΩ vÔøΩetky ShakespearovÔøΩ sonety, mÔøΩÔøΩete vytvoriÔøΩ podtriedu triedy Document nazvanÔøΩ SonnetDocument a potom vytvoriÔøΩ instanciu SonnetDocument pre kaÔøΩdÔøΩ sonet a predaÔøΩ ju objektu IndexWriter na spracovanie.
\item KaÔøΩdÔøΩ pole v Lucene je pÔøΩr meno/hodnota. Hodnota je textovÔøΩ a mÔøΩÔøΩe byÔøΩ veÔøΩmi dlhÔøΩ reÔøΩazec. 
\item NaprÔøΩklad jeden field v SonnetDocument mÔøΩÔøΩe byÔøΩ pomenovanÔøΩ PATH s hodnotou ukladajÔøΩcou cestu a nÔøΩzov sÔøΩboru, ktorÔøΩ obsahuje text sonetu a druhÔøΩ field mÔøΩÔøΩe byÔøΩ pomenovanÔøΩ CONTENT a bude obsahovaÔøΩ velÔøΩ text sonetu.
\item Pri pouÔøΩÔøΩvanÔøΩ veÔøΩkÔøΩho bloku textu ponÔøΩka rozhranie Lucene metÔøΩdu, ktorÔøΩ berie na vstupe objekt java.io.Reader (vstup sÔøΩbor, alebo URL sÔøΩboru)
\item VÔøΩetky tieto polia mÔøΩÔøΩu byÔøΩ uloÔøΩenÔøΩ priamo v indexe, ale ak uÔøΩ mÔøΩte dÔøΩta uloÔøΩenÔøΩ naprÔøΩklad v osobitnÔøΩch textovÔøΩch sÔøΩboroch nemusÔøΩte ÔøΩiadaÔøΩ Lucene aby ich ukladal znovu. (Sonety uloÔøΩenÔøΩ v textovÔøΩch sÔøΩboroch).
\end{itemize}

\section{A Lucene index}
\begin{itemize}
\item Index logicky pozostÔøΩva z objektov Dokumentov a Fieldov. Lucene vÔøΩak fyzicky spravuje index ako mnoÔøΩinu sÔøΩborov v adresÔøΩri.
\item PoÔøΩas procesu indexovania sa Dokumenty predanÔøΩ objektu IndexWriter spÔøΩjajÔøΩ do segmentov. Tieto segmenty sÔøΩ fyzicky zapÔøΩsanÔøΩ v sÔøΩboroch v danom indexovom adresÔøΩri.
\item KaÔøΩdÔøΩ segment je osobitnÔøΩ samostatnÔøΩ index.

\includegraphics[width=60mm]{depth.png}

\item Na obrÔøΩzku vidÔøΩte, ÔøΩe Lucene spravuje indexy vo forme segmentov (sub-indexov). ViacerÔøΩ segmenty sa mÔøΩÔøΩu spÔøΩjaÔøΩ poÔøΩas ÔøΩdrÔøΩby alebo optimalizÔøΩcie indexovania.

\item V dokumentÔøΩciÔøΩ je napÔøΩsanÔøΩ ako sa dÔøΩ zvÔøΩÔøΩiÔøΩ vÔøΩkon Lucene nastavenÔøΩm rÔøΩznych systÔøΩmovÔøΩch premennÔøΩch. (java heap memory, ...)
\end{itemize}

\newpage

\section{PraktickÔøΩ prÔøΩklad}

PraktickÔøΩ ukÔøΩÔøΩka prÔøΩce Lucene na vzorke ShakespearovÔøΩch sonetov.
NasledujÔøΩci program bude schopnÔøΩ vykonÔøΩvaÔøΩ tieto funkcie:

\begin{itemize}
\item VytvoriÔøΩ Lucene index pre vÔøΩetky sonety
\item DovoÔøΩuje uÔøΩÔøΩvateÔøΩovi dotazovaÔøΩ sa na slovÔøΩ a frÔøΩzy v sonetoch
\end{itemize}

prÔøΩklad pozostÔøΩva z tÔøΩchto Java tried:

\begin{description}
\item[SonnetDocument] VytvÔøΩra Lucene Document pre sonet. NÔøΩzov sÔøΩboru tvorÔøΩ jeden argument a druhÔøΩ tvorÔøΩ textovÔøΩ obsah celÔøΩho sonetu.
\item[Indexer] TÔøΩto trieda obsahuje kÔøΩd, ktorÔøΩ vytvorÔøΩ index pre sonet. PouÔøΩÔøΩva StandardAnalyzer a zapisuje index do adresÔøΩra nazvanÔøΩho index.
\item[SonnetFinder] TÔøΩto trieda bude vyhÔøΩadÔøΩvaÔøΩ pomocou indexu na zÔøΩklade uÔøΩÔøΩvateÔøΩskÔøΩho dotazu. PouÔøΩÔøΩva QueryParser a StandardAnalyzer na sparsovanie dotazu a INdexSearcher na vyhÔøΩadÔøΩvanie v indexe.
\end{description}

Z internetu si mÔøΩÔøΩeme stiahnuÔøΩ textovÔøΩ sÔøΩbory jednotlivÔøΩch sonetov, uloÔøΩenÔøΩch pod nÔøΩzvom Sonet?, kde otÔøΩznik zastupuje poradovÔøΩ ÔøΩÔøΩslo sonetu.

\subsection{Adding fields to a Lucene document}
Trieda SonnetDocument vytvÔøΩra Lucene Document, ktorÔøΩ sa puÔøΩije na indexovanie sonetu.

\begin{verbatim}
package uk.co.vsj.lucene;

import java.io.File;
import java.io.Reader;
import java.io.FileInputStream;
import java.io.BufferedReader;
import java.io.InputStreamReader;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
\end{verbatim}

\begin{itemize}
\item StatickÔøΩ metÔøΩda nazvanÔøΩ createDocument() je tovÔøΩreÔøΩ na vÔøΩrobu objektov Document.
\item AtribÔøΩty path a content sÔøΩ pridanÔøΩ metodou Document.add().
\item Field.Text() je pouÔøΩitÔøΩ na vytvorenie atribÔøΩtu content,(parameter je objekt Reader zÔøΩskanÔøΩ zo sÔøΩboru Sonet). Field.Text() vytvÔøΩra obsah, ktorÔøΩ nie je uloÔøΩenÔøΩ v samotnom indexe. (textovÔøΩ obsah uÔøΩ mÔøΩme v textovÔøΩch sÔøΩboroch Sonet?)
\end{itemize}


\begin{verbatim}
public class SonnetDocument {
	public static Document createDocument(File f)
		 throws java.io.FileNotFoundException {
	Document doc = new Document();
	doc.add(Field.Text(ÔøΩpathÔøΩ, f.getPath()));

	FileInputStream is = new FileInputStream(f);
	Reader reader = new BufferedReader(new InputStreamReader(is));
	doc.add(Field.Text(ÔøΩcontentsÔøΩ, reader));
	return doc;
	}

	private SonnetDocument() {}
}
\end{verbatim}

\subsection{Creating the sonnet index}
Trieda Indexer je zodpovednÔøΩ za vytvorenie indexu. Ak je kolekcia sÔøΩborov iba read-only, tak staÔøΩÔøΩ vytvoriÔøΩ jednu instanciu triedy Indexer. Pri pridÔøΩvanÔøΩ a odoberanÔøΩ dÔøΩt do indexu pozri dokumentÔøΩciu.

ZdrojovÔøΩ kÔøΩd triedy Indexer:

\begin{verbatim}
package uk.co.vsj.lucene;

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.index.IndexWriter;
import java.io.File;
import java.io.IOException;
import java.util.Date;
\end{verbatim}

StandardAnalyzer v Lucene analyzuje obsah v anglickom jazyku a odfiltruje nepodstatnÔøΩ slovÔøΩ ako ``a'' a ``the'' poÔøΩas vytvÔøΩtania indexu. Lucene obsahuje taktieÔøΩ analyzatori pre ruskÔøΩ a nemeckÔøΩ jazyk.

postup indexÔøΩcie:
\begin{itemize}
\item VytvoriÔøΩ instanciu IndexWriter s konkrÔøΩtnym analyzÔøΩtorom.
\item Postupne povytvÔøΩraÔøΩ objekty Document a pridaÔøΩ ich do indexu metodou IndexWriter.addDocument().
\item ZavolaÔøΩ metÔøΩdu IndexWriter.optimize na optimalizovanie indexu.
\item ZatvoriÔøΩ instanciu IndexWriter.
\end{itemize}

IndexÔøΩcia potrvÔøΩ na modernejÔøΩÔøΩch pc iba pÔøΩr sekÔøΩnd.

\begin{verbatim}
public class Indexer {
	public static void createIndex() {
		Date start = new Date();
		IndexWriter writer = null;
		try {
			writer = new IndexWriter(ÔøΩindexÔøΩ, new StandardAnalyzer(), true);
			for( int i=1; i< 155; i++) {
				writer.addDocument(SonnetDocument.createDocument(new File(
			ÔøΩSonnetÔøΩ + i)));
			}
			writer.optimize();

			Date end = new Date();

			System.out.println(ÔøΩIndexing took ÔøΩ + (end.getTime() - start.getTime()) +
			 ÔøΩ millisecondsÔøΩ);
		} catch (IOException e) {
			e.printStackTrace();
		}
		finally {
			if (writer != null)
			try {
				writer.close();
			} catch (Exception ex) {}
		}
	}
}
\end{verbatim}

\subsection{Searching the index}
Dotazy mÔøΩÔøΩeme vykonÔøΩvaÔøΩ buÔøΩ pomocou Lucene API alebo pomocou prÔøΩkazovÔøΩho riadku.

\begin{verbatim}
package uk.co.vsj.lucene;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.queryParser.QueryParser;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Searcher;
\end{verbatim}

\begin{itemize}
\item QueryParser vytvÔøΩra dotazy pre Lucene. Pri vytvorenÔøΩ QueryParseru potrebujete rovnako StandardAnalyzer.
\item IndexSearcher poskytuje metÔøΩdu search(). MetÔøΩda vracia objekt Hits, ÔøΩo je zoznam dokumentov, v ktorÔøΩch sa naÔøΩlo dotazovanÔøΩ slovo.
\item Pomocou Hits.doc(n) mÔøΩÔøΩeme pristupovaÔøΩ k dokumentom a zobraziÔøΩ ich nÔøΩzov, aby sme vedeli v ktorÔøΩch sonetoch sa nachÔøΩdza dane slovo.
\end{itemize}

\begin{verbatim}
public class SonnetFinder {
	public static void searchFor(
		String queryText) {
		try {
			Searcher searcher = new	IndexSearcher(ÔøΩindexÔøΩ);
			Analyzer analyzer = new	StandardAnalyzer();

			Query query = QueryParser.parse(queryText, ÔøΩcontentsÔøΩ, analyzer);
			System.out.println(ÔøΩSearching for: ÔøΩ + query.toString(ÔøΩcontentsÔøΩ));

			Hits hits = searcher.search(query);
			int hitsCount =	hits.length();
			System.out.println(hitsCount + ÔøΩ matching sonnets foundÔøΩ);
			for (int i = 0;	i < hitsCount; i++) {
				Document doc = hits.doc(i);
				String path = doc.get(ÔøΩpathÔøΩ);
				if (path != null) {
					System.out.println(i + ÔøΩ. ÔøΩ + path);
				}
			} // of for

		searcher.close();

		} catch (Exception e) {
				e.printStackTrace();
		}
	}
}
\end{verbatim}

\newpage 

\subsection{Test}

\begin{verbatim}
You will be prompted to enter your query. Try to look for a word: 

Please enter your query:
deeds

The system returns a list of sonnets that contains the word ÔøΩdeedsÔøΩ. 
You should see 10 matching sonnets, similar to: 

Searching for: deeds
10 matching sonnets found
0. Sonnet37
1. Sonnet90
2. Sonnet94
3. Sonnet111
4. Sonnet121
5. Sonnet34
6. Sonnet61
7. Sonnet69
8. Sonnet131
9. Sonnet150

You can also search for a phrase by using quotation marks, as in: 
Please enter your query:
ÔøΩthy deedsÔøΩ

This time, ÔøΩthy deedsÔøΩ only matches 3 sonnets: 

Searching for: ÔøΩthy deedsÔøΩ
3 matching sonnets found
0. Sonnet69
1. Sonnet131
2. Sonnet150

It is also possible to search using the AND operator. 
The following query makes sure the sonnet contains both
the phrase ÔøΩthy deedsÔøΩ and ÔøΩmy mindÔøΩ: 

Please enter your query:
ÔøΩthy deedsÔøΩ AND ÔøΩmy mindÔøΩ

You will match only one single sonnet using the above query: 

Searching for: +ÔøΩthy deedsÔøΩ +ÔøΩmy mindÔøΩ
1 matching sonnets found
0. Sonnet150
\end{verbatim}

\newpage

\section{ZÔøΩver}

\begin{itemize}
\item Lucene je jednodchÔøΩ na nauÔøΩenie.
\item Ak potrebujete rieÔøΩenie na indexÔøΩciu textu, Lucene je ten pravÔøΩ nÔøΩstroj.
\item VyuÔøΩitie vo viacerÔøΩch oblastiach, kde by RDBMS bolo zbytoÔøΩnÔøΩ.
\end{itemize}

\section{Zdroje}

\begin{itemize}
\item http://lucene.apache.org
\item http://www.vsj.co.uk/java/display.asp?id=474
\end{itemize}

\end{document}